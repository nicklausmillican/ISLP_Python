{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMH6KPPvHCYB9pxAEBcm0al",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicklausmillican/ISLP_Python/blob/main/ISLP_Python_Ch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4"
      ],
      "metadata": {
        "id": "v06IlZa-LQKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptual"
      ],
      "metadata": {
        "id": "VaDsnW3QCYIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        "Using a little bit of algebra, prove that (4.2)\n",
        "$$p(X) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}$$\n",
        "is equivalent to (4.3).\n",
        "$$\\frac{p(X)}{1-p(X)} = e^{X\\beta}$$\n",
        "\n",
        "(NOTE: The book uses slightly different exponents, $\\beta_0 + \\beta_1 X$, but use this abbreviated version for ease.)\n",
        "\n",
        "In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n",
        "\n",
        "#### Answer\n",
        "Let's start with the definition of odds.  For ease, I'll use $p$ instead of $p(X)$.\n",
        "\n",
        "$$odds = \\frac{p}{1-p}$$\n",
        "\n",
        "The logistic function is the *log odds*, which is equal to our linear model.\n",
        "\n",
        "$$log odds = log(\\frac{p}{1-p}) = X\\beta$$.\n",
        "\n",
        "We can retrieve equation (4.3) by exponentiating both sides\n",
        "\n",
        "$$e^{log(\\frac{p}{1-p})} = \\frac{p}{1-p} = e^{X\\beta}$$\n",
        "\n",
        "Now we must solve for $p$ to get equation (4.2).\n",
        "\n",
        "$$p = e^{X\\beta}(1-p) = e^{X\\beta} - pe^{X\\beta} \\rightarrow$$\n",
        "\n",
        "$$p + pe^{X\\beta} = p(1 + X\\beta) \\rightarrow$$\n",
        "\n",
        "$$p = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "jDnvvJi7CcJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "It was stated in the text that classifying an observation to the class for which (4.17)\n",
        "\n",
        "$$p_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2 \\pi \\sigma}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2 \\pi \\sigma}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_l)^2)}$$\n",
        "\n",
        "is largest is equivalent to classifying an observation to the class for which (4.18)\n",
        "\n",
        "$$\\delta_k(x) = x \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} + log(\\pi_k)$$\n",
        "\n",
        "is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a $N(µ_k, σ^2)$ distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.\n",
        "\n",
        "#### Answer\n",
        "The way I'm going to do this is to show that $\\delta_k$ is proportional to the log of the numerator of Bayes' Theorem (when the likelihood is a normal distribution).  Since a log a monotonic transformation, the max of the log-transformed variable corresponds to the max of the untransformed variable.  In Bayes', the value $k$ that gives the largest numerator is the most probable; the denominator cancels out since every expression across all $K$ values uses the same denominator.  Thus, the greatest $\\delta_k$ corresponds to most probable value.\n",
        "\n",
        "Let's start by simplifying the notation.\n",
        "\n",
        "$$p_k(x) = \\frac{\\pi_k \\times f_k(x)}{M}$$\n",
        "\n",
        "where $\\pi_k$ is the prior probability for $k$, $f_k(x)$ is the normally-distributed likelihood\n",
        "\n",
        "$$f_k(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2)$$\n",
        "\n",
        "and $M$ is the marginal distribution\n",
        "\n",
        "$$M = \\sum_{l=1}^K \\pi_l \\times f_l(x) = {\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_l)^2)}$$.\n",
        "\n",
        "If we then take the log of $p_k(x)$, we get\n",
        "\n",
        "$$ln[p_k(X)] = ln[\\pi_k] + ln[f_k(x)] - ln[M]$$.\n",
        "\n",
        "Now I'm going to expand $ln[f_k(x)]$.\n",
        "\n",
        "$$ln[p_k(X)] = ln[\\pi_k] + ln[\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2)] - ln[M]$$\n",
        "\n",
        "$$= ln[\\pi_k] + ln[\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}] -\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2) - ln[M]$$\n",
        "\n",
        "$$= ln[\\pi_k] + ln[(2 \\pi \\sigma^2)^{-1/2}] -\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2) - ln[M]$$\n",
        "\n",
        "$$= ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma^2] + \\frac{(x- \\mu_k)^2}{\\sigma^2}) - lm[M])$$\n",
        "\n",
        "$$= ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma^2] + \\frac{x^2}{\\sigma^2} - \\frac{2 x \\mu_k}{\\sigma^2} + \\frac{\\mu_k^2}{\\sigma^2}) - lm[M])$$\n",
        "\n",
        "$$= ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma^2]) - \\frac{x^2}{2 \\sigma^2} + \\frac{x \\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2}) - lm[M])$$\n",
        "\n",
        "We're in the home stretch.  If we look at our expression, we recognize a few terms that are shared with $\\delta_k$; namely, those that reference $k$.  This should make sense, only values that depend on the $k^{th}$ option can distinguish among $\\delta$ expressions.  Thus, we can drop all terms without reference to $k$, and we get $\\delta_k$.\n",
        "\n",
        "$$\\delta_k = ln[\\pi_k] + \\frac{x \\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2}$$\n",
        "\n",
        "$$= x \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} + log(\\pi_k)$$."
      ],
      "metadata": {
        "id": "u7a2kdGq2qie"
      }
    }
  ]
}