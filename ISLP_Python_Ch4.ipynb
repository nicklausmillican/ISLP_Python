{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMH8o4opN5pyVJVsIbKL8+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicklausmillican/ISLP_Python/blob/main/ISLP_Python_Ch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4"
      ],
      "metadata": {
        "id": "v06IlZa-LQKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptual"
      ],
      "metadata": {
        "id": "VaDsnW3QCYIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        "Using a little bit of algebra, prove that (4.2)\n",
        "$$p(X) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}$$\n",
        "is equivalent to (4.3).\n",
        "$$\\frac{p(X)}{1-p(X)} = e^{X\\beta}$$\n",
        "\n",
        "(NOTE: The book uses slightly different exponents, $\\beta_0 + \\beta_1 X$, but use this abbreviated version for ease.)\n",
        "\n",
        "In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n",
        "\n",
        "#### Answer\n",
        "Let's start with the definition of odds.  For ease, I'll use $p$ instead of $p(X)$.\n",
        "\n",
        "$$odds = \\frac{p}{1-p}$$\n",
        "\n",
        "The logistic function is the *log odds*, which is equal to our linear model.\n",
        "\n",
        "$$log odds = log(\\frac{p}{1-p}) = X\\beta$$.\n",
        "\n",
        "We can retrieve equation (4.3) by exponentiating both sides\n",
        "\n",
        "$$e^{log(\\frac{p}{1-p})} = \\frac{p}{1-p} = e^{X\\beta}$$\n",
        "\n",
        "Now we must solve for $p$ to get equation (4.2).\n",
        "\n",
        "$$p = e^{X\\beta}(1-p) = e^{X\\beta} - pe^{X\\beta} \\rightarrow$$\n",
        "\n",
        "$$p + pe^{X\\beta} = p(1 + X\\beta) \\rightarrow$$\n",
        "\n",
        "$$p = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "jDnvvJi7CcJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "It was stated in the text that classifying an observation to the class for which (4.17)\n",
        "\n",
        "$$p_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2 \\pi \\sigma}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2 \\pi \\sigma}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_l)^2)}$$\n",
        "\n",
        "is largest is equivalent to classifying an observation to the class for which (4.18)\n",
        "\n",
        "$$\\delta_k(x) = x \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} + log(\\pi_k)$$\n",
        "\n",
        "is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a $N(µ_k, σ^2)$ distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.\n",
        "\n",
        "#### Answer\n",
        "The way I'm going to do this is to show that $\\delta_k$ is proportional to the log of the numerator of Bayes' Theorem (when the likelihood is a normal distribution).  Since a log a monotonic transformation, the max of the log-transformed variable corresponds to the max of the untransformed variable.  In Bayes', the value $k$ that gives the largest numerator is the most probable; the denominator cancels out since every expression across all $K$ values uses the same denominator.  Thus, the greatest $\\delta_k$ corresponds to most probable value.\n",
        "\n",
        "Let's start by simplifying the notation.\n",
        "\n",
        "$$p_k(x) = \\frac{\\pi_k \\times f_k(x)}{M}$$\n",
        "\n",
        "where $\\pi_k$ is the prior probability for $k$, $f_k(x)$ is the normally-distributed likelihood\n",
        "\n",
        "$$f_k(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2)$$\n",
        "\n",
        "and $M$ is the marginal distribution\n",
        "\n",
        "$$M = \\sum_{l=1}^K \\pi_l \\times f_l(x) = {\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_l)^2)}$$.\n",
        "\n",
        "If we then take the log of $p_k(x)$, we get\n",
        "\n",
        "$$ln[p_k(X)] = ln[\\pi_k] + ln[f_k(x)] - ln[M]$$.\n",
        "\n",
        "Now I'm going to expand $ln[f_k(x)]$.\n",
        "\n",
        "$$ln[p_k(X)] = ln[\\pi_k] + ln[\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2)] - ln[M]$$\n",
        "\n",
        "$$= ln[\\pi_k] + ln[\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}] -\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2) - ln[M]$$\n",
        "\n",
        "$$= ln[\\pi_k] + ln[(2 \\pi \\sigma^2)^{-1/2}] -\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2) - ln[M]$$\n",
        "\n",
        "$$= ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma^2] + \\frac{(x- \\mu_k)^2}{\\sigma^2}) - lm[M])$$\n",
        "\n",
        "$$= ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma^2] + \\frac{x^2}{\\sigma^2} - \\frac{2 x \\mu_k}{\\sigma^2} + \\frac{\\mu_k^2}{\\sigma^2}) - lm[M])$$\n",
        "\n",
        "$$= ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma^2]) - \\frac{x^2}{2 \\sigma^2} + \\frac{x \\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2}) - lm[M])$$\n",
        "\n",
        "We're in the home stretch.  If we look at our expression, we recognize a few terms that are shared with $\\delta_k$; namely, those that reference $k$.  This should make sense, only values that depend on the $k^{th}$ option can distinguish among $\\delta$ expressions.  Thus, we can drop all terms without reference to $k$, and we get $\\delta_k$.\n",
        "\n",
        "$$\\delta_k = ln[\\pi_k] + \\frac{x \\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2}$$\n",
        "\n",
        "$$= x \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} + log(\\pi_k)$$."
      ],
      "metadata": {
        "id": "u7a2kdGq2qie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3\n",
        "This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class specific mean vector and a class specific covariance matrix. We consider the simple case where $p = 1$; i.e. there is only one feature.\n",
        "\n",
        "Suppose that we have $K$ classes, and that if an observation belongs to the $k^{th}$ class then $X$ comes from a one-dimensional normal distribution, $X ∼ N(µ_k, σ^2_k)$. Recall that the density function for the one-dimensional normal distribution is given in (4.16)\n",
        "\n",
        "$$f_k(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} exp(-\\frac{1}{2 \\sigma^2}(x-\\mu_k)^2)$$\n",
        "\n",
        "Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.\n",
        "\n",
        "*Hint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that $σ^2_1 = ... = σ^2_K$.*\n",
        "\n",
        "#### Answer\n",
        "In the solution to Question 2 (above), we found $\\delta_k$ to be proportional to the likelihood of Bayes' theorem when the likelihood is normally distributed.  In that derivation, we assumed that $σ^2_1 = ... = σ^2_K$, and this yielded the $\\delta(x)$ function\n",
        "\n",
        "$$\\delta_k(x) = x \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} + log(\\pi_k)$$.\n",
        "\n",
        "We say that this $\\delta(x)$ is \"linear\" in $x$ since the highest degree of $x$ is 1, $x=x^1$.  If there was an $x^2$ in there, we'd say that $\\delta(x)$ was \"quadratic\" in $x$; and if an $x^3$, we'd say \"cubic\"...and so on.\n",
        "\n",
        "Here, we're told that if do NOT assume $σ^2_1 = ... = σ^2_K$, then $\\delta(x)$ is quadratic in $x$.  Let's try to derive this; we'll use the same steps as we did for Question 2.\n",
        "\n",
        "Start again by simplifying the notation.\n",
        "\n",
        "$$p_k(x) = \\frac{\\pi_k \\times f_k(x)}{M}$$\n",
        "\n",
        "where $\\pi_k$ is the prior probability for $k$, $f_k(x)$ is the normally-distributed likelihood\n",
        "\n",
        "$$f_k(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}} exp(-\\frac{1}{2 \\sigma_k^2}(x-\\mu_k)^2)$$\n",
        "\n",
        "and $M$ is the marginal distribution\n",
        "\n",
        "$$M = \\sum_{l=1}^K \\pi_l \\times f_l(x) = {\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2 \\pi \\sigma_l^2}} exp(-\\frac{1}{2 \\sigma_l^2}(x-\\mu_l)^2)}$$.\n",
        "\n",
        "This time, we are sure to identify the variance of the $k^{th}$ and $l^{th}$ categories by $\\sigma_k^2$ and $\\sigma_l^2$.\n",
        "\n",
        "If we then take the log of $p_k(x)$, expanding the term for $f_k(x)$, we get\n",
        "\n",
        "$$ln[p_k(x)] = ln[\\pi_k] + ln[\\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}} exp(-\\frac{1}{2 \\sigma_k^2}(x-\\mu_k)^2)] - ln[M]$$\n",
        "\n",
        "$$= ln[\\pi_k] + ln[\\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}}] -\\frac{1}{2 \\sigma_k^2}(x-\\mu_k)^2) - ln[M]$$\n",
        "\n",
        "$$= ln[\\pi_k] + ln[(2 \\pi \\sigma_k^2)^{-1/2}] -\\frac{1}{2 \\sigma_k^2}(x-\\mu_k)^2) - ln[M]$$\n",
        "\n",
        "$$= ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma_k^2] + \\frac{(x- \\mu_k)^2}{\\sigma_k^2}) - ln[M])$$\n",
        "\n",
        "$$= ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma_k^2] + \\frac{x^2}{\\sigma_k^2} - \\frac{2 x \\mu_k}{\\sigma_k^2} + \\frac{\\mu_k^2}{\\sigma_k^2}) - ln[M])$$\n",
        "\n",
        "$$= ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma_k^2]) - \\frac{x^2}{2 \\sigma_k^2} + \\frac{x \\mu_k}{\\sigma_k^2} - \\frac{\\mu_k^2}{2 \\sigma_k^2} - ln[M])$$\n",
        "\n",
        "Now we're basically done.  Let's compare the expression we just derived, for QDA (with $\\sigma_k^2$), to the expression we previously derived, for LDA (with $\\sigma^2$, instead).\n",
        "\n",
        "$$ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma_k^2]) - \\frac{x^2}{2 \\sigma_k^2} + \\frac{x \\mu_k}{\\sigma_k^2} - \\frac{\\mu_k^2}{2 \\sigma_k^2} - ln[M])$$\n",
        "\n",
        "$$vs$$\n",
        "\n",
        "$$ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma^2]) - \\frac{x^2}{2 \\sigma^2} + \\frac{x \\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} - ln[M])$$\n",
        "\n",
        "When we derived the $\\delta(x)$ expression for LDA, we discarded the terms of $ln[p_k(X)]$ that did not reference $x$, leaving\n",
        "\n",
        "$$\\delta_k^{LDA}(x) = ln[\\pi_k] + \\frac{x \\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2}$$\n",
        "\n",
        "Let us do the same for QDA.  We now see that every term except $ln[M]$ references $k$, including $\\frac{x^2}{2 \\sigma_k^2}$, thus making the $\\delta(X)$ function for QDA quadratic in $x$.\n",
        "\n",
        "$$\\delta_k^{QDA}(x) = ln[\\pi_k] - \\frac{1}{2}(ln[2 \\pi \\sigma_k^2]) - \\frac{x^2}{2 \\sigma_k^2} + \\frac{x \\mu_k}{\\sigma_k^2} - \\frac{\\mu_k^2}{2 \\sigma_k^2}$$"
      ],
      "metadata": {
        "id": "XdfvHvpOcqO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Bonus\n",
        "The LDA decision boundary for categories $a$ and $b$ occurs when $\\delta_a = \\delta_b$.  Importantly, $\\delta_k$ for LDA assumed that each $k \\in K$ had the same variance $σ^2_1 = ... = σ^2_K.$.  Under this assumption, we found that the Bayes' decision boundary occurs when\n",
        "\n",
        "$$x \\frac{\\mu_a}{\\sigma^2} - \\frac{\\mu_a^2}{2 \\sigma^2} + log(\\pi_a)\n",
        "= x \\frac{\\mu_b}{\\sigma^2} - \\frac{\\mu_b^2}{2 \\sigma^2} + log(\\pi_b)$$\n",
        "\n",
        "$$\\rightarrow x \\frac{\\mu_a}{\\sigma^2} - x \\frac{\\mu_b}{\\sigma^2} + log(\\pi_a)\n",
        "= \\frac{\\mu_a^2}{2 \\sigma^2} - \\frac{\\mu_b^2}{2 \\sigma^2} + log(\\pi_b)$$\n",
        "\n",
        "$$= \\frac{x(\\mu_a - \\mu_b)}{\\sigma^2} + log(\\pi_a) = \\frac{1}{2} \\frac{\\mu^2_a - \\mu^2_b}{\\sigma^2} + log(\\pi_b)$$\n",
        "\n",
        "$$\\rightarrow 2x(\\mu_a - \\mu_b) + 2log(\\pi_a)\\sigma^2 = \\mu^2_a - \\mu^2_b + 2log(\\pi_b)\\sigma^2$$\n",
        "\n",
        "$$\\rightarrow x = \\frac{\\mu^2_a - \\mu^2_b}{2(\\mu_a - \\mu_b)} + \\frac{log(\\pi_b / \\pi_a)\\sigma^2}{(\\mu_a - \\mu_b)}$$\n",
        "\n",
        "If $\\pi_a = \\pi_b$, then this simplifies to\n",
        "\n",
        "$$x = \\frac{\\mu^2_a - \\mu^2_b}{2(\\mu_a - \\mu_b)}$$\n",
        "\n",
        "But in quadratic discriminant analysis (QDA), we allow each category to have individual variances.  How will this change our derivation for the decision boundary?\n",
        "\n",
        "$$ln[\\pi_a] - \\frac{1}{2}(ln[2 \\pi \\sigma_a^2]) - \\frac{x^2}{2 \\sigma_a^2} + \\frac{x \\mu_a}{\\sigma_a^2} - \\frac{\\mu_a^2}{2 \\sigma_a^2}\n",
        "= ln[\\pi_b] - \\frac{1}{2}(ln[2 \\pi \\sigma_b^2]) - \\frac{x^2}{2 \\sigma_b^2} + \\frac{x \\mu_b}{\\sigma_b^2} - \\frac{\\mu_b^2}{2 \\sigma_b^2}$$\n",
        "\n",
        "$$\\rightarrow ln[\\pi_a] - ln[\\pi_b] + \\frac{1}{2}(ln[2 \\pi \\sigma_b^2]) - \\frac{1}{2}(ln[2 \\pi \\sigma_a^2]) + \\frac{x^2}{2 \\sigma_b^2} - \\frac{x^2}{2 \\sigma_a^2} + \\frac{x \\mu_a}{\\sigma_a^2} - \\frac{x \\mu_b}{\\sigma_b^2} + \\frac{\\mu_b^2}{2 \\sigma_b^2} - \\frac{\\mu_a^2}{2 \\sigma_a^2} = 0$$\n",
        "\n",
        "$$\\rightarrow 2ln[\\frac{\\pi_a}{\\pi_b}] + ln[\\frac{\\sigma^2_b}{\\sigma^2_a}] + \\frac{x^2}{\\sigma_b^2} - \\frac{x^2}{\\sigma_a^2} + \\frac{2x \\mu_a}{\\sigma_a^2} - \\frac{2x \\mu_b}{\\sigma_b^2} + \\frac{\\mu_b^2}{\\sigma_b^2} - \\frac{\\mu_a^2}{\\sigma_a^2} = 0$$\n",
        "\n",
        "$$\\rightarrow ln[\\frac{\\pi^2_a \\sigma^2_b}{\\pi^2_b \\sigma^2_a}] + x^2(\\frac{\\sigma^2_a - \\sigma^2_b}{\\sigma^2_a \\sigma^2_b}) + x(\\frac{2(\\mu_a \\sigma^2_b - \\mu_b \\sigma^2_a)}{\\sigma^2_a \\sigma^2_b}) + \\frac{\\mu^2_b \\sigma^2_a - \\mu^2_a \\sigma^2_b}{\\sigma^2_a \\sigma^2_b} = 0$$\n",
        "\n",
        "$$\\rightarrow x^2(\\frac{\\sigma^2_a - \\sigma^2_b}{\\sigma^2_a \\sigma^2_b}) + x(\\frac{2(\\mu_a \\sigma^2_b - \\mu_b \\sigma^2_a)}{\\sigma^2_a \\sigma^2_b}) + (\\frac{\\mu^2_b \\sigma^2_a - \\mu^2_a \\sigma^2_b}{\\sigma^2_a \\sigma^2_b} + ln[\\frac{\\pi^2_a \\sigma^2_b}{\\pi^2_b \\sigma^2_a}]) = 0$$\n",
        "\n",
        "$$\\rightarrow Ax^2 + Bx + C = 0$$\n",
        "\n",
        "Thus, we have a quadratic equation. In other words, our decision boundary is also quadratic in $x$ for QDA.  We can solve this with the quadratic formula:\n",
        "\n",
        "$$x = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}$$\n",
        "\n",
        "I won't solve that here; it's just an exercise in algebra."
      ],
      "metadata": {
        "id": "utWJQEwJZoqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check out the Desmos Demo\n",
        "I made a demo of this in Desmos.  You can check it out [here](https://www.desmos.com/calculator/qbdkdiuipt).\n",
        "\n",
        "Start by noticing what happens with the priors are equal (in Desmos, $p_a = p_b$).  The LDA decision boundary is exactly where the two distributions meet.  As you move $p_a$ to become higher, $p_b$ gets lower and so the decision boundary encroaches on b's territory.\n",
        "\n",
        "When the variances are equal (in Desmos, $s_a = s_b$), there is not QDA boundary.  Only when they are not equal do these emerge.  At the same time, the LDA boundary no longer intersects where a and b are equal (when $p_a = p_b$).\n",
        "\n",
        "With just 1 predictor variable, the QDA boundaries are a little difficult to map to the behavior of the normal distributions.  Hopefully, a 2-predictor variable question will show up later."
      ],
      "metadata": {
        "id": "U2S1a0hcliUy"
      }
    }
  ]
}